const generatedBibEntries = {
    "Chung2021MachineLearning": {
        "abstract": "With increasing deployment of machine learning systems in various real-world tasks, there is a greater need for accurate quantification of predictive uncertainty. While the common goal in uncertainty quantification (UQ) in machine learning is to approximate the true distribution of the target data, many works in UQ tend to be disjoint in the evaluation metrics utilized, and disparate implementations for each metric lead to numerical results that are not directly comparable across different works. To address this, we introduce Uncertainty Toolbox, an open-source python library that helps to assess, visualize, and improve UQ. Uncertainty Toolbox additionally provides pedagogical resources, such as a glossary of key terms and an organized collection of key paper references. We hope that this toolbox is useful for accelerating and uniting research efforts in uncertainty in machine learning.",
        "author": "Chung, Youngseog and Char, Ian and Guo, Han and Schneider, Jeff and Neiswanger, Willie",
        "doi": "https://doi.org/10.48550/arXiv.2109.10254",
        "journal": "arXiv preprint arXiv:2109.10254",
        "keywords": "type: article;Uncertainty Quantification (UQ)\u200b\u200b;\u200b\u200bCalibration Metrics\u200b\u200b;Proper Scoring Rules\u200b\u200b;Probabilistic Neural Networks (PNNs)\u200b\u200b;Adversarial Group Calibration",
        "number": "",
        "publisher": "arXiv",
        "series": "arXiv",
        "title": "Uncertainty toolbox: an open-source library for assessing, visualizing, and improving uncertainty quantification",
        "type": "article",
        "url": "https://doi.org/10.48550/arXiv.2109.10254",
        "volume": "",
        "year": "2021"
    },
    "Dai2024Kernel": {
        "abstract": "Kernel ridge regression (KRR) is widely used for nonparametric regression over reproducing kernel Hilbert spaces. It offers powerful modeling capabilities at the cost of significant computational costs, which typically require O(n3) computational time and O(n2) storage space, with the sample size n. We introduce a novel framework of multi-layer kernel machines that approximate KRR by employing a multi-layer structure and random features, and study how the optimal number of random features and layer sizes can be chosen while still preserving the minimax optimality of the approximate KRR estimate. For various classes of random features, including those corresponding to Gaussian and Matern kernels, we prove that multi-layer kernel machines can achieve O(n2log2n) computational time and O(nlog2n) storage space, and yield fast and minimax optimal approximations to the KRR estimate for nonparametric regression. Moreover, we construct uncertainty quantification for multi-layer kernel machines by using conformal prediction techniques with robust coverage properties. The analysis and theoretical predictions are supported by simulations and real data examples.",
        "author": "Dai, Xiaowu and Zhong, Huiying",
        "doi": "https://doi.org/10.48550/arXiv.2403.09907",
        "journal": "arXiv preprint arXiv:2403.09907",
        "keywords": "type: article;Nonparametric regression; random feature; kernel method; multi-layer structure;uncertainty quantification",
        "number": "",
        "publisher": "arXiv",
        "series": "arXiv",
        "title": "Multi-Layer Kernel Machines: Fast and Optimal Nonparametric Regression with Uncertainty Quantification",
        "type": "article",
        "url": "https://doi.org/10.48550/arXiv.2403.09907",
        "volume": "",
        "year": "2024 ,"
    },
    "Ghoshal2022Uncertainty": {
        "abstract": "Estimated uncertainty by approximate posteriors in Bayesian neural networks are prone to miscalibration, which leads to overconfident predictions in critical tasks that have a clear asymmetric cost or significant losses. Here, we extend the approximate inference for the loss-calibrated Bayesian framework to dropweights based Bayesian neural networks by maximising expected utility over a model posterior to calibrate uncertainty in deep learning. Furthermore, we show that decisions informed by loss-calibrated uncertainty can improve diagnostic performance to a greater extent than straightforward alternatives. We propose Maximum Uncertainty Calibration Error (MUCE) as a metric to measure calibrated confidence, in addition to its prediction especially for high-risk applications, where the goal is to minimise the worst-case deviation between error and estimated uncertainty. In experiments, we show the correlation between error in prediction and estimated uncertainty by interpreting Wasserstein distance as the accuracy of prediction. We evaluated the effectiveness of our approach to detecting Covid-19 from X-Ray images. Experimental results show that our method reduces miscalibration considerably, without impacting the models accuracy and improves reliability of computer-based diagnostics.",
        "author": "Ghoshal, Biraja and Tucker, Allan",
        "doi": "https://doi.org/10.48550/arXiv.2206.07795",
        "journal": "arXiv preprint arXiv:2206.07795",
        "keywords": "type: article;Loss-Calibrated Neural Network;Model Calibration;Uncertainty;Estimation;Maximum Uncertainty Calibration Error;Covid-19",
        "number": "",
        "publisher": "arXiv",
        "series": "arXiv",
        "title": "On calibrated model uncertainty in deep learning",
        "type": "article",
        "url": "https://doi.org/10.48550/arXiv.2206.07795",
        "volume": "",
        "year": "2022 ,"
    },
    "H\u00fcllermeier2021Uncertainty": {
        "abstract": "The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.",
        "author": "H\u00fcllermeier, E. and Waegeman, W",
        "doi": "https://doi.org/10.1007/s10994-021-05946-3",
        "journal": "Machine learning",
        "keywords": "type: article;Uncertainty;Probability;Epistemic uncertainty;Version space learning;Bayesian inference;Calibration;Ensembles;Gaussian processes;Deep neural networks;Likelihood-based methods;Credal sets and classifers;Conformal prediction;Set-valued prediction;Generative models",
        "number": "3",
        "publisher": "Springer",
        "series": "Springer",
        "title": "Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods",
        "type": "article",
        "url": "https://doi.org/10.1007/s10994-021-05946-3",
        "volume": "110",
        "year": "2021 ,"
    },
    "Kabir2018Uncertainty": {
        "abstract": "Uncertainty quantification plays a critical role in the process of decision making and optimization in many fields of science and engineering. The field has gained an overwhelming attention among researchers in recent years resulting in an arsenal of different methods. Probabilistic forecasting and in particular prediction intervals (PIs) are one of the techniques most widely used in the literature for uncertainty quantification. Researchers have reported studies of uncertainty quantification in critical applications such as medical diagnostics, bioinformatics, renewable energies, and power grids. The purpose of this survey paper is to comprehensively study neural network-based methods for construction of prediction intervals. It will cover how PIs are constructed, optimized, and applied for decision-making in presence of uncertainties. Also, different criteria for unbiased PI evaluation are investigated. The paper also provides some guidelines for further research in the field of neural network-based uncertainty quantification.",
        "author": "Kabir, H. M. Dipu and Khosravi, Abbas and Hosen, Mohammad Anwar and Nahavandi, Saeid",
        "doi": "10.1109/ACCESS.2018.2836917",
        "journal": "IEEE Access",
        "keywords": "type: article;Uncertainty;Probability density function;Artificial neural networks;Probabilistic logic;Forecasting;Upper bound;Prediction interval;uncertainty quantification;heteroscedastic uncertainty;neural network;forecast;time series data;regression;probability",
        "number": "",
        "publisher": "IEEE",
        "series": "IEEE",
        "title": "Neural Network-Based Uncertainty Quantification: A Survey of Methodologies and Applications",
        "type": "article",
        "url": "https://doi.org/10.1109/access.2018.2836917",
        "volume": "6",
        "year": "2018 ,"
    },
    "Michelmore2018DNN": {
        "abstract": "A rise in popularity of Deep Neural Networks (DNNs), attributed to more powerful GPUs and widely available datasets, has seen them being increasingly used within safety-critical domains. One such domain, self-driving, has benefited from significant performance improvements, with millions of miles having been driven with no human intervention. Despite this, crashes and erroneous behaviours still occur, in part due to the complexity of verifying the correctness of DNNs and a lack of safety guarantees.In this paper, we demonstrate how quantitative measures of uncertainty can be extracted in real-time, and their quality evaluated in end-to-end controllers for self-driving cars. To this end we utilise a recent method for gathering approximate uncertainty information from DNNs without changing the network's architecture. We propose evaluation techniques for the uncertainty on two separate architectures which use the uncertainty to predict crashes up to five seconds in advance. We find that mutual information, a measure of uncertainty in classification networks, is a promising indicator of forthcoming crashes.",
        "author": "Michelmore, Rhiannon and Kwiatkowska, Marta and Gal, Yarin",
        "doi": "https://doi.org/10.48550/arXiv.1811.06817",
        "journal": "arXiv preprint arXiv:1811.06817",
        "keywords": "type: article;Uncertainty Quantification\u200b\u200b;End-to-End Autonomous Driving\u200b\u200b;Deep Neural Networks (DNNs)\u200b\u200b;Mutual Information\u200b\u200b;Crash Prediction",
        "number": "",
        "publisher": "arXiv",
        "series": "arXiv",
        "title": "Evaluating uncertainty quantification in end-to-end autonomous driving control",
        "type": "article",
        "url": "https://doi.org/10.48550/arXiv.1811.06817",
        "volume": "",
        "year": "2018 ,"
    },
    "Pawlowski2017Neural": {
        "abstract": "Modern neural networks tend to be overconfident on unseen, noisy or incorrectly labelled data and do not produce meaningful uncertainty measures. Bayesian deep learning aims to address this shortcoming with variational approximations (such as Bayes by Backprop or Multiplicative Normalising Flows). However, current approaches have limitations regarding flexibility and scalability. We introduce Bayes by Hypernet (BbH), a new method of variational approximation that interprets hypernetworks as implicit distributions. It naturally uses neural networks to model arbitrarily complex distributions and scales to modern deep learning architectures. In our experiments, we demonstrate that our method achieves competitive accuracies and predictive uncertainties on MNIST and a CIFAR5 task, while being the most robust against adversarial attacks.",
        "author": "Pawlowski, Nick and Brock, Andrew and Lee, Matthew CH and Rajchl, Martin and Glocker, Ben",
        "doi": "https://doi.org/10.48550/arXiv.1711.01297",
        "journal": "arXiv preprint arXiv:1711.01297",
        "keywords": "type: article;Neural networks;Bayesian;Multiplicative Normalising Flows",
        "number": "",
        "publisher": "arXiv",
        "series": "arXiv",
        "title": "Implicit weight uncertainty in neural networks",
        "type": "article",
        "url": "https://doi.org/10.48550/arXiv.1711.01297",
        "volume": "",
        "year": "2017 ,"
    },
    "Russell2021DeepLearning": {
        "abstract": "Deep learning has the potential to dramatically impact navigation and tracking state estimation problems critical to autonomous vehicles and robotics. Measurement uncertainties in state estimation systems based on Kalman and other Bayes filters are typically assumed to be a fixed covariance matrix. This assumption is risky, particularly for \u201cblack box\u201d deep learning models, in which uncertainty can vary dramatically and unexpectedly. Accurate quantification of multivariate uncertainty will allow for the full potential of deep learning to be used more safely and reliably in these applications. We show how to model multivariate uncertainty for regression problems with neural networks, incorporating both aleatoric and epistemic sources of heteroscedastic uncertainty. We train a deep uncertainty covariance matrix model in two ways: directly using a multivariate Gaussian density loss function and indirectly using end-to-end training through a Kalman filter. We experimentally show in a visual tracking problem the large impact that accurate multivariate uncertainty quantification can have on the Kalman filter performance for both in-domain and out-of-domain evaluation data. We additionally show, in a challenging visual odometry problem, how end-to-end filter training can allow uncertainty predictions to compensate for filter weaknesses.",
        "author": "Russell, Rebecca L and Reale, Christopher",
        "doi": "https://doi.org/10.1109/TNNLS.2021.3086757",
        "journal": "IEEE Transactions on Neural Networks and Learning Systems",
        "keywords": "type: article;Covariance matrices;deep learning;22 filters;neural networks;uncertainty",
        "number": "12",
        "publisher": "IEEE",
        "series": "IEEE",
        "title": "Multivariate uncertainty in deep learning",
        "type": "article",
        "url": "https://doi.org/10.1109/TNNLS.2021.3086757",
        "volume": "33",
        "year": "2021 ,"
    },
    "Weytjens2022Uncertainty": {
        "abstract": "The inability of artificial neural networks to assess the uncertainty of their predictions is an impediment to their widespread use. We distinguish two types of learnable uncertainty: model uncertainty due to a lack of training data and noise-induced observational uncertainty. Bayesian neural networks use solid mathematical foundations to learn the model uncertainties of their predictions. The observational uncertainty can be calculated by adding one layer to these networks and augmenting their loss functions. Our contribution is to apply these uncertainty concepts to predictive process monitoring tasks to train uncertainty-based models to predict the remaining time and outcomes. Our experiments show that uncertainty estimates allow more and less accurate predictions to be differentiated and confidence intervals to be constructed in both regression and classification tasks. These conclusions remain true even in early stages of running processes. Moreover, the deployed techniques are fast and produce more accurate predictions. The learned uncertainty could increase users\u2019 confidence in their process prediction systems, promote better cooperation between humans and these systems, and enable earlier implementations with smaller datasets.",
        "author": "Weytjens, Hans and De Weerdt, Jochen",
        "doi": "https://doi.org/10.1016/j.asoc.2022.109134",
        "journal": "Applied Soft Computing",
        "keywords": "type: article;Predictive process monitoring;Remaining time prediction;Outcome prediction;Artificial neural networks;Bayesian neural networks;Uncertainty",
        "number": "",
        "publisher": "Elsevier",
        "series": "Elsevier",
        "title": "Learning uncertainty with artificial neural networks for predictive process monitoring",
        "type": "article",
        "url": "https://doi.org/10.1016/j.asoc.2022.109134",
        "volume": "125",
        "year": "2022 ,"
    },
    "Wu2024Classical": {
        "abstract": "Classical parameter-space Bayesian inference for Bayesian neural networks (BNNs) suffers from several unresolved prior issues, such as knowledge encoding intractability and pathological behaviours in deep networks, which can lead to improper posterior inference. To address these issues, functional Bayesian inference has recently been proposed leveraging functional priors, such as the emerging functional variational inference. In addition to variational methods, stochastic gradient Markov Chain Monte Carlo (MCMC) is another scalable and effective inference method for BNNs to asymptotically generate samples from the true posterior by simulating continuous dynamics. However, existing MCMC methods perform solely in parameter space and inherit the unresolved prior issues, while extending these dynamics to function space is a non-trivial undertaking. In this paper, we introduce novel functional MCMC schemes, including stochastic gradient versions, based on newly designed diffusion dynamics that can incorporate more informative functional priors. Moreover, we prove that the stationary measure of these functional dynamics is the target posterior over functions. Our functional MCMC schemes demonstrate improved performance in both predictive accuracy and uncertainty quantification on several tasks compared to naive parameter-space MCMC and functional variational inference.",
        "author": "Wu, Mengjing and Xuan, Junyu and Lu, Jie",
        "doi": "https://doi.org/10.48550/arXiv.2409.16632",
        "journal": "arXiv preprint arXiv:2409.16632",
        "keywords": "type: article;Bayesian Neural Networks;Functional MCMC;Stochastic Gradient MCMC;Gaussian Process Priors;Uncertainty Quantification",
        "number": "",
        "publisher": "arXiv",
        "series": "arXiv",
        "title": "Functional Stochastic Gradient MCMC for Bayesian Neural Networks",
        "type": "article",
        "url": "https://doi.org/10.48550/arXiv.2409.16632",
        "volume": "",
        "year": "2024 ,"
    }
};